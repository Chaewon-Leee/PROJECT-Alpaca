{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpaca"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2ff4c3524c54ebe9e86797c65293187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_ckpt = \"chainyo/alpaca-lora-7b\"\n",
    "\n",
    "Alpaca_tokenizer = LlamaTokenizer.from_pretrained(model_ckpt)\n",
    "Alpaca_model = LlamaForCausalLM.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=31999)\n",
       "    (layers): ModuleList(\n",
       "      (0): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (1): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (2): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (3): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (4): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (5): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (6): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (7): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (8): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (9): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (10): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (11): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (12): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (13): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (14): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (15): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (16): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (17): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (18): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (19): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (20): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (21): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (22): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (23): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (24): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (25): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (26): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (27): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (28): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (29): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (30): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (31): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Alpaca_model.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alpaca Prompt input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def generate_prompt(instruction: str, input_ctxt: str = None) -> str:\n",
    "    if input_ctxt:\n",
    "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "              ### Instruction:\n",
    "              {instruction}\n",
    "\n",
    "              ### Input:\n",
    "              {input_ctxt}\n",
    "\n",
    "              ### Response:\"\"\"\n",
    "    else:\n",
    "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "              ### Instruction:\n",
    "              {instruction}\n",
    "\n",
    "              ### Response:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    temperature=0.2,\n",
    "    top_p=0.75,\n",
    "    top_k=40,\n",
    "    num_beams=4,\n",
    "    max_new_tokens=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 22:15:10.582160: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "              ### Instruction:\n",
      "              What is the meaning of life?\n",
      "\n",
      "              ### Response:\n",
      "              The meaning of life is to find purpose and meaning in one's own life. It is a journey of self-discovery and self-actualization. It is a process of exploring one's values, beliefs, and goals. It is a search for meaning and fulfillment in one's own life.\n"
     ]
    }
   ],
   "source": [
    "instruction = \"What is the meaning of life?\"\n",
    "input_ctxt = None\n",
    "\n",
    "prompt = generate_prompt(instruction, input_ctxt)\n",
    "input_ids = Alpaca_tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "input_ids = input_ids.to(Alpaca_model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = Alpaca_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=generation_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "    )\n",
    "\n",
    "response = Alpaca_tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Alpaca Cleaned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/chaewon/.cache/huggingface/datasets/yahma___json/yahma--alpaca-cleaned-fe377fcd47a14100/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee873c81e074046bc78fa51927ecd20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output'],\n",
       "        num_rows: 51760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "alpaca_cleaned_dataset = load_dataset(\"yahma/alpaca-cleaned\")\n",
    "alpaca_cleaned_dataset # Dataset 클래스 객체 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def df_generate_prompt(x) -> str:\n",
    "    if x[\"input\"]:\n",
    "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "              ### Instruction:\n",
    "              {x[\"instruction\"]}\n",
    "\n",
    "              ### Input:\n",
    "              {x[\"input\"]}\n",
    "\n",
    "              ### Response:\"\"\"\n",
    "    else:\n",
    "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "                  ### Instruction:\n",
    "                  {x[\"instruction\"]}\n",
    "\n",
    "                  ### Response:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>prompt_input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Give three tips for staying healthy.</td>\n",
       "      <td></td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the three primary colors?</td>\n",
       "      <td></td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Describe the structure of an atom.</td>\n",
       "      <td></td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How can we reduce air pollution?</td>\n",
       "      <td></td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pretend you are a project manager of a constru...</td>\n",
       "      <td></td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51755</th>\n",
       "      <td>You will be given a piece of text about an eve...</td>\n",
       "      <td>Text: John went out for a walk with his dog Ro...</td>\n",
       "      <td>Below is an instruction that describes a task,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51756</th>\n",
       "      <td>You will be given a paragraph of text with var...</td>\n",
       "      <td>Text: Michael Jordan is an American former pro...</td>\n",
       "      <td>Below is an instruction that describes a task,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51757</th>\n",
       "      <td>You will be given a piece of text about an eve...</td>\n",
       "      <td>Text: A tree fell over in the wind and caused ...</td>\n",
       "      <td>Below is an instruction that describes a task,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51758</th>\n",
       "      <td>I will give you a list of steps.  You need to ...</td>\n",
       "      <td>Steps: ['She takes out her books', 'The teache...</td>\n",
       "      <td>Below is an instruction that describes a task,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51759</th>\n",
       "      <td>Given a piece of text, you need to output whet...</td>\n",
       "      <td>Text: The sky was very cloudy today.</td>\n",
       "      <td>Below is an instruction that describes a task,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51760 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             instruction  \\\n",
       "0                   Give three tips for staying healthy.   \n",
       "1                     What are the three primary colors?   \n",
       "2                     Describe the structure of an atom.   \n",
       "3                       How can we reduce air pollution?   \n",
       "4      Pretend you are a project manager of a constru...   \n",
       "...                                                  ...   \n",
       "51755  You will be given a piece of text about an eve...   \n",
       "51756  You will be given a paragraph of text with var...   \n",
       "51757  You will be given a piece of text about an eve...   \n",
       "51758  I will give you a list of steps.  You need to ...   \n",
       "51759  Given a piece of text, you need to output whet...   \n",
       "\n",
       "                                                   input  \\\n",
       "0                                                          \n",
       "1                                                          \n",
       "2                                                          \n",
       "3                                                          \n",
       "4                                                          \n",
       "...                                                  ...   \n",
       "51755  Text: John went out for a walk with his dog Ro...   \n",
       "51756  Text: Michael Jordan is an American former pro...   \n",
       "51757  Text: A tree fell over in the wind and caused ...   \n",
       "51758  Steps: ['She takes out her books', 'The teache...   \n",
       "51759               Text: The sky was very cloudy today.   \n",
       "\n",
       "                                            prompt_input  \n",
       "0      Below is an instruction that describes a task....  \n",
       "1      Below is an instruction that describes a task....  \n",
       "2      Below is an instruction that describes a task....  \n",
       "3      Below is an instruction that describes a task....  \n",
       "4      Below is an instruction that describes a task....  \n",
       "...                                                  ...  \n",
       "51755  Below is an instruction that describes a task,...  \n",
       "51756  Below is an instruction that describes a task,...  \n",
       "51757  Below is an instruction that describes a task,...  \n",
       "51758  Below is an instruction that describes a task,...  \n",
       "51759  Below is an instruction that describes a task,...  \n",
       "\n",
       "[51760 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# pd.options.display.max_colwidth = 2000\n",
    "\n",
    "df = pd.DataFrame(alpaca_cleaned_dataset[\"train\"])\n",
    "train_df = df.drop(columns=[\"output\"])\n",
    "\n",
    "train_df[\"prompt_input\"] = train_df.apply(df_generate_prompt, axis=1)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(x):\n",
    "  return Alpaca_tokenizer(x, return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>prompt_input</th>\n",
       "      <th>input_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Give three tips for staying healthy.</td>\n",
       "      <td></td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "      <td>[[tensor(0), tensor(13866), tensor(338), tenso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the three primary colors?</td>\n",
       "      <td></td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "      <td>[[tensor(0), tensor(13866), tensor(338), tenso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Describe the structure of an atom.</td>\n",
       "      <td></td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "      <td>[[tensor(0), tensor(13866), tensor(338), tenso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How can we reduce air pollution?</td>\n",
       "      <td></td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "      <td>[[tensor(0), tensor(13866), tensor(338), tenso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pretend you are a project manager of a constru...</td>\n",
       "      <td></td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "      <td>[[tensor(0), tensor(13866), tensor(338), tenso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51755</th>\n",
       "      <td>You will be given a piece of text about an eve...</td>\n",
       "      <td>Text: John went out for a walk with his dog Ro...</td>\n",
       "      <td>Below is an instruction that describes a task,...</td>\n",
       "      <td>[[tensor(0), tensor(13866), tensor(338), tenso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51756</th>\n",
       "      <td>You will be given a paragraph of text with var...</td>\n",
       "      <td>Text: Michael Jordan is an American former pro...</td>\n",
       "      <td>Below is an instruction that describes a task,...</td>\n",
       "      <td>[[tensor(0), tensor(13866), tensor(338), tenso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51757</th>\n",
       "      <td>You will be given a piece of text about an eve...</td>\n",
       "      <td>Text: A tree fell over in the wind and caused ...</td>\n",
       "      <td>Below is an instruction that describes a task,...</td>\n",
       "      <td>[[tensor(0), tensor(13866), tensor(338), tenso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51758</th>\n",
       "      <td>I will give you a list of steps.  You need to ...</td>\n",
       "      <td>Steps: ['She takes out her books', 'The teache...</td>\n",
       "      <td>Below is an instruction that describes a task,...</td>\n",
       "      <td>[[tensor(0), tensor(13866), tensor(338), tenso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51759</th>\n",
       "      <td>Given a piece of text, you need to output whet...</td>\n",
       "      <td>Text: The sky was very cloudy today.</td>\n",
       "      <td>Below is an instruction that describes a task,...</td>\n",
       "      <td>[[tensor(0), tensor(13866), tensor(338), tenso...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51760 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             instruction  \\\n",
       "0                   Give three tips for staying healthy.   \n",
       "1                     What are the three primary colors?   \n",
       "2                     Describe the structure of an atom.   \n",
       "3                       How can we reduce air pollution?   \n",
       "4      Pretend you are a project manager of a constru...   \n",
       "...                                                  ...   \n",
       "51755  You will be given a piece of text about an eve...   \n",
       "51756  You will be given a paragraph of text with var...   \n",
       "51757  You will be given a piece of text about an eve...   \n",
       "51758  I will give you a list of steps.  You need to ...   \n",
       "51759  Given a piece of text, you need to output whet...   \n",
       "\n",
       "                                                   input  \\\n",
       "0                                                          \n",
       "1                                                          \n",
       "2                                                          \n",
       "3                                                          \n",
       "4                                                          \n",
       "...                                                  ...   \n",
       "51755  Text: John went out for a walk with his dog Ro...   \n",
       "51756  Text: Michael Jordan is an American former pro...   \n",
       "51757  Text: A tree fell over in the wind and caused ...   \n",
       "51758  Steps: ['She takes out her books', 'The teache...   \n",
       "51759               Text: The sky was very cloudy today.   \n",
       "\n",
       "                                            prompt_input  \\\n",
       "0      Below is an instruction that describes a task....   \n",
       "1      Below is an instruction that describes a task....   \n",
       "2      Below is an instruction that describes a task....   \n",
       "3      Below is an instruction that describes a task....   \n",
       "4      Below is an instruction that describes a task....   \n",
       "...                                                  ...   \n",
       "51755  Below is an instruction that describes a task,...   \n",
       "51756  Below is an instruction that describes a task,...   \n",
       "51757  Below is an instruction that describes a task,...   \n",
       "51758  Below is an instruction that describes a task,...   \n",
       "51759  Below is an instruction that describes a task,...   \n",
       "\n",
       "                                               input_ids  \n",
       "0      [[tensor(0), tensor(13866), tensor(338), tenso...  \n",
       "1      [[tensor(0), tensor(13866), tensor(338), tenso...  \n",
       "2      [[tensor(0), tensor(13866), tensor(338), tenso...  \n",
       "3      [[tensor(0), tensor(13866), tensor(338), tenso...  \n",
       "4      [[tensor(0), tensor(13866), tensor(338), tenso...  \n",
       "...                                                  ...  \n",
       "51755  [[tensor(0), tensor(13866), tensor(338), tenso...  \n",
       "51756  [[tensor(0), tensor(13866), tensor(338), tenso...  \n",
       "51757  [[tensor(0), tensor(13866), tensor(338), tenso...  \n",
       "51758  [[tensor(0), tensor(13866), tensor(338), tenso...  \n",
       "51759  [[tensor(0), tensor(13866), tensor(338), tenso...  \n",
       "\n",
       "[51760 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"input_ids\"] = train_df[\"prompt_input\"].map(tokenize)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def generate_response(x):\n",
    "  input_ids = x.to(Alpaca_model.device)\n",
    "    \n",
    "  with torch.no_grad():\n",
    "      outputs = Alpaca_model.generate(\n",
    "          input_ids=input_ids,\n",
    "          generation_config=generation_config,\n",
    "          return_dict_in_generate=True,\n",
    "          output_scores=True,\n",
    "      )\n",
    "\n",
    "  return Alpaca_tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [13:58<00:00, 83.82s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "train_df_10 = train_df.iloc[:10]\n",
    "train_df_10[\"response\"] = train_df_10[\"input_ids\"].progress_map(generate_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>prompt_input</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Give three tips for staying healthy.</td>\n",
       "      <td></td>\n",
       "      <td>Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n                  ### Instruction:\\n                  Give three tips for staying healthy.\\n\\n                  ### Response:</td>\n",
       "      <td>[[tensor(0), tensor(13866), tensor(338), tensor(385), tensor(15278), tensor(393), tensor(16612), tensor(263), tensor(3414), tensor(29889), tensor(14350), tensor(263), tensor(2933), tensor(393), tensor(7128), tensor(2486), tensor(1614), tensor(2167), tensor(278), tensor(2009), tensor(29889), tensor(13), tensor(13), tensor(462), tensor(29871), tensor(835), tensor(2799), tensor(4080), tensor(29901), tensor(13), tensor(462), tensor(29871), tensor(25538), tensor(2211), tensor(25562), tensor(363), tensor(7952), tensor(292), tensor(9045), tensor(29891), tensor(29889), tensor(13), tensor(13), tensor(462), tensor(29871), tensor(835), tensor(13291), tensor(29901)]]</td>\n",
       "      <td>Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n                  ### Instruction:\\n                  Give three tips for staying healthy.\\n\\n                  ### Response:\\n                  1. Eat a balanced diet.\\n                  2. Exercise regularly.\\n                  3. Get enough sleep.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the three primary colors?</td>\n",
       "      <td></td>\n",
       "      <td>Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n                  ### Instruction:\\n                  What are the three primary colors?\\n\\n                  ### Response:</td>\n",
       "      <td>[[tensor(0), tensor(13866), tensor(338), tensor(385), tensor(15278), tensor(393), tensor(16612), tensor(263), tensor(3414), tensor(29889), tensor(14350), tensor(263), tensor(2933), tensor(393), tensor(7128), tensor(2486), tensor(1614), tensor(2167), tensor(278), tensor(2009), tensor(29889), tensor(13), tensor(13), tensor(462), tensor(29871), tensor(835), tensor(2799), tensor(4080), tensor(29901), tensor(13), tensor(462), tensor(29871), tensor(1724), tensor(526), tensor(278), tensor(2211), tensor(7601), tensor(11955), tensor(29973), tensor(13), tensor(13), tensor(462), tensor(29871), tensor(835), tensor(13291), tensor(29901)]]</td>\n",
       "      <td>Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n                  ### Instruction:\\n                  What are the three primary colors?\\n\\n                  ### Response:\\n                  The three primary colors are red, blue, and yellow.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Describe the structure of an atom.</td>\n",
       "      <td></td>\n",
       "      <td>Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n                  ### Instruction:\\n                  Describe the structure of an atom.\\n\\n                  ### Response:</td>\n",
       "      <td>[[tensor(0), tensor(13866), tensor(338), tensor(385), tensor(15278), tensor(393), tensor(16612), tensor(263), tensor(3414), tensor(29889), tensor(14350), tensor(263), tensor(2933), tensor(393), tensor(7128), tensor(2486), tensor(1614), tensor(2167), tensor(278), tensor(2009), tensor(29889), tensor(13), tensor(13), tensor(462), tensor(29871), tensor(835), tensor(2799), tensor(4080), tensor(29901), tensor(13), tensor(462), tensor(29871), tensor(20355), tensor(915), tensor(278), tensor(3829), tensor(310), tensor(385), tensor(12301), tensor(29889), tensor(13), tensor(13), tensor(462), tensor(29871), tensor(835), tensor(13291), tensor(29901)]]</td>\n",
       "      <td>Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n                  ### Instruction:\\n                  Describe the structure of an atom.\\n\\n                  ### Response:\\n                  An atom is composed of a nucleus, which is made up of protons and neutrons, and electrons, which are arranged in shells around the nucleus. The nucleus is made up of positively charged protons and uncharged neutrons. The electrons are negatively charged particles that orbit the nucleus.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How can we reduce air pollution?</td>\n",
       "      <td></td>\n",
       "      <td>Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n                  ### Instruction:\\n                  How can we reduce air pollution?\\n\\n                  ### Response:</td>\n",
       "      <td>[[tensor(0), tensor(13866), tensor(338), tensor(385), tensor(15278), tensor(393), tensor(16612), tensor(263), tensor(3414), tensor(29889), tensor(14350), tensor(263), tensor(2933), tensor(393), tensor(7128), tensor(2486), tensor(1614), tensor(2167), tensor(278), tensor(2009), tensor(29889), tensor(13), tensor(13), tensor(462), tensor(29871), tensor(835), tensor(2799), tensor(4080), tensor(29901), tensor(13), tensor(462), tensor(29871), tensor(1128), tensor(508), tensor(591), tensor(10032), tensor(4799), tensor(21180), tensor(918), tensor(29973), tensor(13), tensor(13), tensor(462), tensor(29871), tensor(835), tensor(13291), tensor(29901)]]</td>\n",
       "      <td>Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n                  ### Instruction:\\n                  How can we reduce air pollution?\\n\\n                  ### Response:\\n                  We can reduce air pollution by switching to renewable energy sources, such as solar and wind power. We can also reduce our reliance on fossil fuels by using public transportation, carpooling, and biking. We can also reduce air pollution by planting trees and shrubs, which absorb carbon dioxide and other pollutants.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pretend you are a project manager of a construction company. Describe a time when you had to make a difficult decision.</td>\n",
       "      <td></td>\n",
       "      <td>Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n                  ### Instruction:\\n                  Pretend you are a project manager of a construction company. Describe a time when you had to make a difficult decision.\\n\\n                  ### Response:</td>\n",
       "      <td>[[tensor(0), tensor(13866), tensor(338), tensor(385), tensor(15278), tensor(393), tensor(16612), tensor(263), tensor(3414), tensor(29889), tensor(14350), tensor(263), tensor(2933), tensor(393), tensor(7128), tensor(2486), tensor(1614), tensor(2167), tensor(278), tensor(2009), tensor(29889), tensor(13), tensor(13), tensor(462), tensor(29871), tensor(835), tensor(2799), tensor(4080), tensor(29901), tensor(13), tensor(462), tensor(29871), tensor(349), tensor(2267), tensor(355), tensor(366), tensor(526), tensor(263), tensor(2060), tensor(8455), tensor(310), tensor(263), tensor(7632), tensor(5001), tensor(29889), tensor(20355), tensor(915), tensor(263), tensor(931), tensor(746), tensor(366), tensor(750), tensor(304), tensor(1207), tensor(263), tensor(5189), tensor(10608), tensor(29889), tensor(13), tensor(13), tensor(462), tensor(29871), tensor(835), tensor(13291), tensor(29901)]]</td>\n",
       "      <td>Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n                  ### Instruction:\\n                  Pretend you are a project manager of a construction company. Describe a time when you had to make a difficult decision.\\n\\n                  ### Response:\\nAs a project manager of a construction company, I had to make difficult decisions on a regular basis. One example was when I had to decide whether to continue with a project that was running over budget. I had to weigh the pros and cons of continuing with the project, as well as the potential risks of not completing it. In the end, I decided to continue with the project, as I believed that the benefits of completing it outweighed the risks of not doing so.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Write a concise summary of the following:\\n\"Commodore 64 (commonly known as the C64 or CBM 64) was manufactured by Commodore Business Machine (CBM) in August 1982 with a starting price of $595. It was an 8-bit home computer with remarkable market success. Between 1983-1986, C64 sales amounted to about 17 million units sold, making them the best-selling single personal computer model of all time in 1983-1986. \\n\\nAdditionally, the Commodore 64 dominated the market with between 30% and 40% share and 2 million units sold per year, outselling the IBM PC clones, Apple Computers, and Atari computers. Adding to their success, Sam Tramiel (former Atari president), during an interview in 1989, said they were building 400,000 C64s a month for a couple of years. \"</td>\n",
       "      <td></td>\n",
       "      <td>Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n                  ### Instruction:\\n                  Write a concise summary of the following:\\n\"Commodore 64 (commonly known as the C64 or CBM 64) was manufactured by Commodore Business Machine (CBM) in August 1982 with a starting price of $595. It was an 8-bit home computer with remarkable market success. Between 1983-1986, C64 sales amounted to about 17 million units sold, making them the best-selling single personal computer model of all time in 1983-1986. \\n\\nAdditionally, the Commodore 64 dominated the market with between 30% and 40% share and 2 million units sold per year, outselling the IBM PC clones, Apple Computers, and Atari computers. Adding to their success, Sam Tramiel (former Atari president), during an interview in 1989, said they were building 400,000 C64s a month for a couple of years. \"\\n\\n                  ### Response:</td>\n",
       "      <td>[[tensor(0), tensor(13866), tensor(338), tensor(385), tensor(15278), tensor(393), tensor(16612), tensor(263), tensor(3414), tensor(29889), tensor(14350), tensor(263), tensor(2933), tensor(393), tensor(7128), tensor(2486), tensor(1614), tensor(2167), tensor(278), tensor(2009), tensor(29889), tensor(13), tensor(13), tensor(462), tensor(29871), tensor(835), tensor(2799), tensor(4080), tensor(29901), tensor(13), tensor(462), tensor(29871), tensor(14350), tensor(263), tensor(3022), tensor(895), tensor(15837), tensor(310), tensor(278), tensor(1494), tensor(29901), tensor(13), tensor(29908), tensor(1523), tensor(1545), tensor(487), tensor(29871), tensor(29953), tensor(29946), tensor(313), tensor(2055), tensor(6194), tensor(2998), tensor(408), tensor(278), tensor(315), tensor(29953), tensor(29946), tensor(470), tensor(315), tensor(29933), tensor(29924), tensor(29871), tensor(29953), tensor(29946), tensor(29897), tensor(471), tensor(12012), tensor(2955), tensor(491), tensor(422), tensor(1545), tensor(487), tensor(15197), tensor(6189), tensor(313), tensor(21685), tensor(29924), tensor(29897), tensor(297), tensor(3111), tensor(29871), tensor(29896), tensor(29929), tensor(29947), tensor(29906), tensor(411), tensor(263), tensor(6257), tensor(8666), tensor(310), tensor(395), tensor(29945), tensor(29929), tensor(29945), tensor(29889), tensor(739), tensor(471), tensor(385), tensor(29871), ...]]</td>\n",
       "      <td>Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n                  ### Instruction:\\n                  Write a concise summary of the following:\\n\"Commodore 64 (commonly known as the C64 or CBM 64) was manufactured by Commodore Business Machine (CBM) in August 1982 with a starting price of $595. It was an 8-bit home computer with remarkable market success. Between 1983-1986, C64 sales amounted to about 17 million units sold, making them the best-selling single personal computer model of all time in 1983-1986. \\n\\nAdditionally, the Commodore 64 dominated the market with between 30% and 40% share and 2 million units sold per year, outselling the IBM PC clones, Apple Computers, and Atari computers. Adding to their success, Sam Tramiel (former Atari president), during an interview in 1989, said they were building 400,000 C64s a month for a couple of years. \"\\n\\n                  ### Response:\\nThe Commodore 64 (commonly known as the C64 or CBM 64) was manufactured by Commodore Business Machine (CBM) in August 1982 with a starting price of $595. It was an 8-bit home computer with remarkable market success. Between 1983-1986, C64 sales amounted to about 17 million units sold, making them the best-selling single personal computer model of all time in 1983-1986. Additionally, the Commodore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Explain why the following fraction is equivalent to 1/4</td>\n",
       "      <td>4/16</td>\n",
       "      <td>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n              ### Instruction:\\n              Explain why the following fraction is equivalent to 1/4\\n\\n              ### Input:\\n              4/16\\n\\n              ### Response:</td>\n",
       "      <td>[[tensor(0), tensor(13866), tensor(338), tensor(385), tensor(15278), tensor(393), tensor(16612), tensor(263), tensor(3414), tensor(29892), tensor(3300), tensor(2859), tensor(411), tensor(385), tensor(1881), tensor(393), tensor(8128), tensor(4340), tensor(3030), tensor(29889), tensor(14350), tensor(263), tensor(2933), tensor(393), tensor(7128), tensor(2486), tensor(1614), tensor(2167), tensor(278), tensor(2009), tensor(29889), tensor(13), tensor(13), tensor(795), tensor(835), tensor(2799), tensor(4080), tensor(29901), tensor(13), tensor(795), tensor(12027), tensor(7420), tensor(2020), tensor(278), tensor(1494), tensor(15958), tensor(338), tensor(7126), tensor(304), tensor(29871), tensor(29896), tensor(29914), tensor(29946), tensor(13), tensor(13), tensor(795), tensor(835), tensor(10567), tensor(29901), tensor(13), tensor(1669), tensor(29946), tensor(29914), tensor(29896), tensor(29953), tensor(13), tensor(13), tensor(795), tensor(835), tensor(13291), tensor(29901)]]</td>\n",
       "      <td>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n              ### Instruction:\\n              Explain why the following fraction is equivalent to 1/4\\n\\n              ### Input:\\n              4/16\\n\\n              ### Response:\\n              The fraction 4/16 is equivalent to 1/4 because it can be simplified to 1/4 by dividing both numerator and denominator by 4.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Write a short story in third person narration about a protagonist who has to make an important career decision.</td>\n",
       "      <td></td>\n",
       "      <td>Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n                  ### Instruction:\\n                  Write a short story in third person narration about a protagonist who has to make an important career decision.\\n\\n                  ### Response:</td>\n",
       "      <td>[[tensor(0), tensor(13866), tensor(338), tensor(385), tensor(15278), tensor(393), tensor(16612), tensor(263), tensor(3414), tensor(29889), tensor(14350), tensor(263), tensor(2933), tensor(393), tensor(7128), tensor(2486), tensor(1614), tensor(2167), tensor(278), tensor(2009), tensor(29889), tensor(13), tensor(13), tensor(462), tensor(29871), tensor(835), tensor(2799), tensor(4080), tensor(29901), tensor(13), tensor(462), tensor(29871), tensor(14350), tensor(263), tensor(3273), tensor(5828), tensor(297), tensor(4654), tensor(2022), tensor(15474), tensor(362), tensor(1048), tensor(263), tensor(15572), tensor(391), tensor(1058), tensor(756), tensor(304), tensor(1207), tensor(385), tensor(4100), tensor(6413), tensor(10608), tensor(29889), tensor(13), tensor(13), tensor(462), tensor(29871), tensor(835), tensor(13291), tensor(29901)]]</td>\n",
       "      <td>Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n                  ### Instruction:\\n                  Write a short story in third person narration about a protagonist who has to make an important career decision.\\n\\n                  ### Response:\\nOnce upon a time, there was a young man named John who had to make an important career decision. He had just graduated from college with a degree in computer science, but he wasn't sure if he wanted to pursue a career in that field. \\n\\nJohn had always been interested in business, but he was afraid to take the leap and pursue a career in that field. He was worried that he wouldn't be able to make enough money to support himself and his family. \\n\\nOne day, John decided to take a leap of faith and pursue a career in business. He</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Evaluate this sentence for spelling and grammar mistakes</td>\n",
       "      <td>He finnished his meal and left the resturant</td>\n",
       "      <td>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n              ### Instruction:\\n              Evaluate this sentence for spelling and grammar mistakes\\n\\n              ### Input:\\n              He finnished his meal and left the resturant\\n\\n              ### Response:</td>\n",
       "      <td>[[tensor(0), tensor(13866), tensor(338), tensor(385), tensor(15278), tensor(393), tensor(16612), tensor(263), tensor(3414), tensor(29892), tensor(3300), tensor(2859), tensor(411), tensor(385), tensor(1881), tensor(393), tensor(8128), tensor(4340), tensor(3030), tensor(29889), tensor(14350), tensor(263), tensor(2933), tensor(393), tensor(7128), tensor(2486), tensor(1614), tensor(2167), tensor(278), tensor(2009), tensor(29889), tensor(13), tensor(13), tensor(795), tensor(835), tensor(2799), tensor(4080), tensor(29901), tensor(13), tensor(795), tensor(382), tensor(4387), tensor(403), tensor(445), tensor(10541), tensor(363), tensor(805), tensor(7807), tensor(322), tensor(25437), tensor(28947), tensor(13), tensor(13), tensor(795), tensor(835), tensor(10567), tensor(29901), tensor(13), tensor(795), tensor(940), tensor(1436), tensor(29876), tensor(3276), tensor(670), tensor(592), tensor(284), tensor(322), tensor(2175), tensor(278), tensor(1791), tensor(332), tensor(424), tensor(13), tensor(13), tensor(795), tensor(835), tensor(13291), tensor(29901)]]</td>\n",
       "      <td>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n              ### Instruction:\\n              Evaluate this sentence for spelling and grammar mistakes\\n\\n              ### Input:\\n              He finnished his meal and left the resturant\\n\\n              ### Response:\\n              He finished his meal and left the restaurant.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How did Julius Caesar die?</td>\n",
       "      <td></td>\n",
       "      <td>Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n                  ### Instruction:\\n                  How did Julius Caesar die?\\n\\n                  ### Response:</td>\n",
       "      <td>[[tensor(0), tensor(13866), tensor(338), tensor(385), tensor(15278), tensor(393), tensor(16612), tensor(263), tensor(3414), tensor(29889), tensor(14350), tensor(263), tensor(2933), tensor(393), tensor(7128), tensor(2486), tensor(1614), tensor(2167), tensor(278), tensor(2009), tensor(29889), tensor(13), tensor(13), tensor(462), tensor(29871), tensor(835), tensor(2799), tensor(4080), tensor(29901), tensor(13), tensor(462), tensor(29871), tensor(1128), tensor(1258), tensor(23762), tensor(9243), tensor(26892), tensor(762), tensor(29973), tensor(13), tensor(13), tensor(462), tensor(29871), tensor(835), tensor(13291), tensor(29901)]]</td>\n",
       "      <td>Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n                  ### Instruction:\\n                  How did Julius Caesar die?\\n\\n                  ### Response:\\n                  Julius Caesar was assassinated on the Ides of March in 44 BC.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   instruction  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Give three tips for staying healthy.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           What are the three primary colors?   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Describe the structure of an atom.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             How can we reduce air pollution?   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Pretend you are a project manager of a construction company. Describe a time when you had to make a difficult decision.   \n",
       "5  Write a concise summary of the following:\\n\"Commodore 64 (commonly known as the C64 or CBM 64) was manufactured by Commodore Business Machine (CBM) in August 1982 with a starting price of $595. It was an 8-bit home computer with remarkable market success. Between 1983-1986, C64 sales amounted to about 17 million units sold, making them the best-selling single personal computer model of all time in 1983-1986. \\n\\nAdditionally, the Commodore 64 dominated the market with between 30% and 40% share and 2 million units sold per year, outselling the IBM PC clones, Apple Computers, and Atari computers. Adding to their success, Sam Tramiel (former Atari president), during an interview in 1989, said they were building 400,000 C64s a month for a couple of years. \"   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Explain why the following fraction is equivalent to 1/4   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Write a short story in third person narration about a protagonist who has to make an important career decision.   \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Evaluate this sentence for spelling and grammar mistakes   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   How did Julius Caesar die?   \n",
       "\n",
       "                                          input  \\\n",
       "0                                                 \n",
       "1                                                 \n",
       "2                                                 \n",
       "3                                                 \n",
       "4                                                 \n",
       "5                                                 \n",
       "6                                          4/16   \n",
       "7                                                 \n",
       "8  He finnished his meal and left the resturant   \n",
       "9                                                 \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        prompt_input  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n                  ### Instruction:\\n                  Give three tips for staying healthy.\\n\\n                  ### Response:   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n                  ### Instruction:\\n                  What are the three primary colors?\\n\\n                  ### Response:   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n                  ### Instruction:\\n                  Describe the structure of an atom.\\n\\n                  ### Response:   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n                  ### Instruction:\\n                  How can we reduce air pollution?\\n\\n                  ### Response:   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n                  ### Instruction:\\n                  Pretend you are a project manager of a construction company. Describe a time when you had to make a difficult decision.\\n\\n                  ### Response:   \n",
       "5  Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n                  ### Instruction:\\n                  Write a concise summary of the following:\\n\"Commodore 64 (commonly known as the C64 or CBM 64) was manufactured by Commodore Business Machine (CBM) in August 1982 with a starting price of $595. It was an 8-bit home computer with remarkable market success. Between 1983-1986, C64 sales amounted to about 17 million units sold, making them the best-selling single personal computer model of all time in 1983-1986. \\n\\nAdditionally, the Commodore 64 dominated the market with between 30% and 40% share and 2 million units sold per year, outselling the IBM PC clones, Apple Computers, and Atari computers. Adding to their success, Sam Tramiel (former Atari president), during an interview in 1989, said they were building 400,000 C64s a month for a couple of years. \"\\n\\n                  ### Response:   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n              ### Instruction:\\n              Explain why the following fraction is equivalent to 1/4\\n\\n              ### Input:\\n              4/16\\n\\n              ### Response:   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n                  ### Instruction:\\n                  Write a short story in third person narration about a protagonist who has to make an important career decision.\\n\\n                  ### Response:   \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n              ### Instruction:\\n              Evaluate this sentence for spelling and grammar mistakes\\n\\n              ### Input:\\n              He finnished his meal and left the resturant\\n\\n              ### Response:   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n                  ### Instruction:\\n                  How did Julius Caesar die?\\n\\n                  ### Response:   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   input_ids  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    [[tensor(0), tensor(13866), tensor(338), tensor(385), tensor(15278), tensor(393), tensor(16612), tensor(263), tensor(3414), tensor(29889), tensor(14350), tensor(263), tensor(2933), tensor(393), tensor(7128), tensor(2486), tensor(1614), tensor(2167), tensor(278), tensor(2009), tensor(29889), tensor(13), tensor(13), tensor(462), tensor(29871), tensor(835), tensor(2799), tensor(4080), tensor(29901), tensor(13), tensor(462), tensor(29871), tensor(25538), tensor(2211), tensor(25562), tensor(363), tensor(7952), tensor(292), tensor(9045), tensor(29891), tensor(29889), tensor(13), tensor(13), tensor(462), tensor(29871), tensor(835), tensor(13291), tensor(29901)]]   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  [[tensor(0), tensor(13866), tensor(338), tensor(385), tensor(15278), tensor(393), tensor(16612), tensor(263), tensor(3414), tensor(29889), tensor(14350), tensor(263), tensor(2933), tensor(393), tensor(7128), tensor(2486), tensor(1614), tensor(2167), tensor(278), tensor(2009), tensor(29889), tensor(13), tensor(13), tensor(462), tensor(29871), tensor(835), tensor(2799), tensor(4080), tensor(29901), tensor(13), tensor(462), tensor(29871), tensor(1724), tensor(526), tensor(278), tensor(2211), tensor(7601), tensor(11955), tensor(29973), tensor(13), tensor(13), tensor(462), tensor(29871), tensor(835), tensor(13291), tensor(29901)]]   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     [[tensor(0), tensor(13866), tensor(338), tensor(385), tensor(15278), tensor(393), tensor(16612), tensor(263), tensor(3414), tensor(29889), tensor(14350), tensor(263), tensor(2933), tensor(393), tensor(7128), tensor(2486), tensor(1614), tensor(2167), tensor(278), tensor(2009), tensor(29889), tensor(13), tensor(13), tensor(462), tensor(29871), tensor(835), tensor(2799), tensor(4080), tensor(29901), tensor(13), tensor(462), tensor(29871), tensor(20355), tensor(915), tensor(278), tensor(3829), tensor(310), tensor(385), tensor(12301), tensor(29889), tensor(13), tensor(13), tensor(462), tensor(29871), tensor(835), tensor(13291), tensor(29901)]]   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    [[tensor(0), tensor(13866), tensor(338), tensor(385), tensor(15278), tensor(393), tensor(16612), tensor(263), tensor(3414), tensor(29889), tensor(14350), tensor(263), tensor(2933), tensor(393), tensor(7128), tensor(2486), tensor(1614), tensor(2167), tensor(278), tensor(2009), tensor(29889), tensor(13), tensor(13), tensor(462), tensor(29871), tensor(835), tensor(2799), tensor(4080), tensor(29901), tensor(13), tensor(462), tensor(29871), tensor(1128), tensor(508), tensor(591), tensor(10032), tensor(4799), tensor(21180), tensor(918), tensor(29973), tensor(13), tensor(13), tensor(462), tensor(29871), tensor(835), tensor(13291), tensor(29901)]]   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   [[tensor(0), tensor(13866), tensor(338), tensor(385), tensor(15278), tensor(393), tensor(16612), tensor(263), tensor(3414), tensor(29889), tensor(14350), tensor(263), tensor(2933), tensor(393), tensor(7128), tensor(2486), tensor(1614), tensor(2167), tensor(278), tensor(2009), tensor(29889), tensor(13), tensor(13), tensor(462), tensor(29871), tensor(835), tensor(2799), tensor(4080), tensor(29901), tensor(13), tensor(462), tensor(29871), tensor(349), tensor(2267), tensor(355), tensor(366), tensor(526), tensor(263), tensor(2060), tensor(8455), tensor(310), tensor(263), tensor(7632), tensor(5001), tensor(29889), tensor(20355), tensor(915), tensor(263), tensor(931), tensor(746), tensor(366), tensor(750), tensor(304), tensor(1207), tensor(263), tensor(5189), tensor(10608), tensor(29889), tensor(13), tensor(13), tensor(462), tensor(29871), tensor(835), tensor(13291), tensor(29901)]]   \n",
       "5  [[tensor(0), tensor(13866), tensor(338), tensor(385), tensor(15278), tensor(393), tensor(16612), tensor(263), tensor(3414), tensor(29889), tensor(14350), tensor(263), tensor(2933), tensor(393), tensor(7128), tensor(2486), tensor(1614), tensor(2167), tensor(278), tensor(2009), tensor(29889), tensor(13), tensor(13), tensor(462), tensor(29871), tensor(835), tensor(2799), tensor(4080), tensor(29901), tensor(13), tensor(462), tensor(29871), tensor(14350), tensor(263), tensor(3022), tensor(895), tensor(15837), tensor(310), tensor(278), tensor(1494), tensor(29901), tensor(13), tensor(29908), tensor(1523), tensor(1545), tensor(487), tensor(29871), tensor(29953), tensor(29946), tensor(313), tensor(2055), tensor(6194), tensor(2998), tensor(408), tensor(278), tensor(315), tensor(29953), tensor(29946), tensor(470), tensor(315), tensor(29933), tensor(29924), tensor(29871), tensor(29953), tensor(29946), tensor(29897), tensor(471), tensor(12012), tensor(2955), tensor(491), tensor(422), tensor(1545), tensor(487), tensor(15197), tensor(6189), tensor(313), tensor(21685), tensor(29924), tensor(29897), tensor(297), tensor(3111), tensor(29871), tensor(29896), tensor(29929), tensor(29947), tensor(29906), tensor(411), tensor(263), tensor(6257), tensor(8666), tensor(310), tensor(395), tensor(29945), tensor(29929), tensor(29945), tensor(29889), tensor(739), tensor(471), tensor(385), tensor(29871), ...]]   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                        [[tensor(0), tensor(13866), tensor(338), tensor(385), tensor(15278), tensor(393), tensor(16612), tensor(263), tensor(3414), tensor(29892), tensor(3300), tensor(2859), tensor(411), tensor(385), tensor(1881), tensor(393), tensor(8128), tensor(4340), tensor(3030), tensor(29889), tensor(14350), tensor(263), tensor(2933), tensor(393), tensor(7128), tensor(2486), tensor(1614), tensor(2167), tensor(278), tensor(2009), tensor(29889), tensor(13), tensor(13), tensor(795), tensor(835), tensor(2799), tensor(4080), tensor(29901), tensor(13), tensor(795), tensor(12027), tensor(7420), tensor(2020), tensor(278), tensor(1494), tensor(15958), tensor(338), tensor(7126), tensor(304), tensor(29871), tensor(29896), tensor(29914), tensor(29946), tensor(13), tensor(13), tensor(795), tensor(835), tensor(10567), tensor(29901), tensor(13), tensor(1669), tensor(29946), tensor(29914), tensor(29896), tensor(29953), tensor(13), tensor(13), tensor(795), tensor(835), tensor(13291), tensor(29901)]]   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   [[tensor(0), tensor(13866), tensor(338), tensor(385), tensor(15278), tensor(393), tensor(16612), tensor(263), tensor(3414), tensor(29889), tensor(14350), tensor(263), tensor(2933), tensor(393), tensor(7128), tensor(2486), tensor(1614), tensor(2167), tensor(278), tensor(2009), tensor(29889), tensor(13), tensor(13), tensor(462), tensor(29871), tensor(835), tensor(2799), tensor(4080), tensor(29901), tensor(13), tensor(462), tensor(29871), tensor(14350), tensor(263), tensor(3273), tensor(5828), tensor(297), tensor(4654), tensor(2022), tensor(15474), tensor(362), tensor(1048), tensor(263), tensor(15572), tensor(391), tensor(1058), tensor(756), tensor(304), tensor(1207), tensor(385), tensor(4100), tensor(6413), tensor(10608), tensor(29889), tensor(13), tensor(13), tensor(462), tensor(29871), tensor(835), tensor(13291), tensor(29901)]]   \n",
       "8                                                                                                                                                                                                                                                                                                                                                        [[tensor(0), tensor(13866), tensor(338), tensor(385), tensor(15278), tensor(393), tensor(16612), tensor(263), tensor(3414), tensor(29892), tensor(3300), tensor(2859), tensor(411), tensor(385), tensor(1881), tensor(393), tensor(8128), tensor(4340), tensor(3030), tensor(29889), tensor(14350), tensor(263), tensor(2933), tensor(393), tensor(7128), tensor(2486), tensor(1614), tensor(2167), tensor(278), tensor(2009), tensor(29889), tensor(13), tensor(13), tensor(795), tensor(835), tensor(2799), tensor(4080), tensor(29901), tensor(13), tensor(795), tensor(382), tensor(4387), tensor(403), tensor(445), tensor(10541), tensor(363), tensor(805), tensor(7807), tensor(322), tensor(25437), tensor(28947), tensor(13), tensor(13), tensor(795), tensor(835), tensor(10567), tensor(29901), tensor(13), tensor(795), tensor(940), tensor(1436), tensor(29876), tensor(3276), tensor(670), tensor(592), tensor(284), tensor(322), tensor(2175), tensor(278), tensor(1791), tensor(332), tensor(424), tensor(13), tensor(13), tensor(795), tensor(835), tensor(13291), tensor(29901)]]   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                [[tensor(0), tensor(13866), tensor(338), tensor(385), tensor(15278), tensor(393), tensor(16612), tensor(263), tensor(3414), tensor(29889), tensor(14350), tensor(263), tensor(2933), tensor(393), tensor(7128), tensor(2486), tensor(1614), tensor(2167), tensor(278), tensor(2009), tensor(29889), tensor(13), tensor(13), tensor(462), tensor(29871), tensor(835), tensor(2799), tensor(4080), tensor(29901), tensor(13), tensor(462), tensor(29871), tensor(1128), tensor(1258), tensor(23762), tensor(9243), tensor(26892), tensor(762), tensor(29973), tensor(13), tensor(13), tensor(462), tensor(29871), tensor(835), tensor(13291), tensor(29901)]]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             response  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n                  ### Instruction:\\n                  Give three tips for staying healthy.\\n\\n                  ### Response:\\n                  1. Eat a balanced diet.\\n                  2. Exercise regularly.\\n                  3. Get enough sleep.  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n                  ### Instruction:\\n                  What are the three primary colors?\\n\\n                  ### Response:\\n                  The three primary colors are red, blue, and yellow.  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n                  ### Instruction:\\n                  Describe the structure of an atom.\\n\\n                  ### Response:\\n                  An atom is composed of a nucleus, which is made up of protons and neutrons, and electrons, which are arranged in shells around the nucleus. The nucleus is made up of positively charged protons and uncharged neutrons. The electrons are negatively charged particles that orbit the nucleus.  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n                  ### Instruction:\\n                  How can we reduce air pollution?\\n\\n                  ### Response:\\n                  We can reduce air pollution by switching to renewable energy sources, such as solar and wind power. We can also reduce our reliance on fossil fuels by using public transportation, carpooling, and biking. We can also reduce air pollution by planting trees and shrubs, which absorb carbon dioxide and other pollutants.  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n                  ### Instruction:\\n                  Pretend you are a project manager of a construction company. Describe a time when you had to make a difficult decision.\\n\\n                  ### Response:\\nAs a project manager of a construction company, I had to make difficult decisions on a regular basis. One example was when I had to decide whether to continue with a project that was running over budget. I had to weigh the pros and cons of continuing with the project, as well as the potential risks of not completing it. In the end, I decided to continue with the project, as I believed that the benefits of completing it outweighed the risks of not doing so.  \n",
       "5  Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n                  ### Instruction:\\n                  Write a concise summary of the following:\\n\"Commodore 64 (commonly known as the C64 or CBM 64) was manufactured by Commodore Business Machine (CBM) in August 1982 with a starting price of $595. It was an 8-bit home computer with remarkable market success. Between 1983-1986, C64 sales amounted to about 17 million units sold, making them the best-selling single personal computer model of all time in 1983-1986. \\n\\nAdditionally, the Commodore 64 dominated the market with between 30% and 40% share and 2 million units sold per year, outselling the IBM PC clones, Apple Computers, and Atari computers. Adding to their success, Sam Tramiel (former Atari president), during an interview in 1989, said they were building 400,000 C64s a month for a couple of years. \"\\n\\n                  ### Response:\\nThe Commodore 64 (commonly known as the C64 or CBM 64) was manufactured by Commodore Business Machine (CBM) in August 1982 with a starting price of $595. It was an 8-bit home computer with remarkable market success. Between 1983-1986, C64 sales amounted to about 17 million units sold, making them the best-selling single personal computer model of all time in 1983-1986. Additionally, the Commodore  \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n              ### Instruction:\\n              Explain why the following fraction is equivalent to 1/4\\n\\n              ### Input:\\n              4/16\\n\\n              ### Response:\\n              The fraction 4/16 is equivalent to 1/4 because it can be simplified to 1/4 by dividing both numerator and denominator by 4.  \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n                  ### Instruction:\\n                  Write a short story in third person narration about a protagonist who has to make an important career decision.\\n\\n                  ### Response:\\nOnce upon a time, there was a young man named John who had to make an important career decision. He had just graduated from college with a degree in computer science, but he wasn't sure if he wanted to pursue a career in that field. \\n\\nJohn had always been interested in business, but he was afraid to take the leap and pursue a career in that field. He was worried that he wouldn't be able to make enough money to support himself and his family. \\n\\nOne day, John decided to take a leap of faith and pursue a career in business. He  \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n              ### Instruction:\\n              Evaluate this sentence for spelling and grammar mistakes\\n\\n              ### Input:\\n              He finnished his meal and left the resturant\\n\\n              ### Response:\\n              He finished his meal and left the restaurant.  \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n                  ### Instruction:\\n                  How did Julius Caesar die?\\n\\n                  ### Response:\\n                  Julius Caesar was assassinated on the Ides of March in 44 BC.  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 2000\n",
    "train_df_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "train_df_10[\"refine_response\"] = train_df_10[\"response\"].map(lambda x: x.split(\"### Response:\")[1].strip())\n",
    "refine_df = train_df_10[[\"instruction\", \"input\", \"refine_response\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>refine_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Give three tips for staying healthy.</td>\n",
       "      <td></td>\n",
       "      <td>1. Eat a balanced diet.\\n                  2. Exercise regularly.\\n                  3. Get enough sleep.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the three primary colors?</td>\n",
       "      <td></td>\n",
       "      <td>The three primary colors are red, blue, and yellow.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Describe the structure of an atom.</td>\n",
       "      <td></td>\n",
       "      <td>An atom is composed of a nucleus, which is made up of protons and neutrons, and electrons, which are arranged in shells around the nucleus. The nucleus is made up of positively charged protons and uncharged neutrons. The electrons are negatively charged particles that orbit the nucleus.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How can we reduce air pollution?</td>\n",
       "      <td></td>\n",
       "      <td>We can reduce air pollution by switching to renewable energy sources, such as solar and wind power. We can also reduce our reliance on fossil fuels by using public transportation, carpooling, and biking. We can also reduce air pollution by planting trees and shrubs, which absorb carbon dioxide and other pollutants.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pretend you are a project manager of a construction company. Describe a time when you had to make a difficult decision.</td>\n",
       "      <td></td>\n",
       "      <td>As a project manager of a construction company, I had to make difficult decisions on a regular basis. One example was when I had to decide whether to continue with a project that was running over budget. I had to weigh the pros and cons of continuing with the project, as well as the potential risks of not completing it. In the end, I decided to continue with the project, as I believed that the benefits of completing it outweighed the risks of not doing so.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Write a concise summary of the following:\\n\"Commodore 64 (commonly known as the C64 or CBM 64) was manufactured by Commodore Business Machine (CBM) in August 1982 with a starting price of $595. It was an 8-bit home computer with remarkable market success. Between 1983-1986, C64 sales amounted to about 17 million units sold, making them the best-selling single personal computer model of all time in 1983-1986. \\n\\nAdditionally, the Commodore 64 dominated the market with between 30% and 40% share and 2 million units sold per year, outselling the IBM PC clones, Apple Computers, and Atari computers. Adding to their success, Sam Tramiel (former Atari president), during an interview in 1989, said they were building 400,000 C64s a month for a couple of years. \"</td>\n",
       "      <td></td>\n",
       "      <td>The Commodore 64 (commonly known as the C64 or CBM 64) was manufactured by Commodore Business Machine (CBM) in August 1982 with a starting price of $595. It was an 8-bit home computer with remarkable market success. Between 1983-1986, C64 sales amounted to about 17 million units sold, making them the best-selling single personal computer model of all time in 1983-1986. Additionally, the Commodore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Explain why the following fraction is equivalent to 1/4</td>\n",
       "      <td>4/16</td>\n",
       "      <td>The fraction 4/16 is equivalent to 1/4 because it can be simplified to 1/4 by dividing both numerator and denominator by 4.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Write a short story in third person narration about a protagonist who has to make an important career decision.</td>\n",
       "      <td></td>\n",
       "      <td>Once upon a time, there was a young man named John who had to make an important career decision. He had just graduated from college with a degree in computer science, but he wasn't sure if he wanted to pursue a career in that field. \\n\\nJohn had always been interested in business, but he was afraid to take the leap and pursue a career in that field. He was worried that he wouldn't be able to make enough money to support himself and his family. \\n\\nOne day, John decided to take a leap of faith and pursue a career in business. He</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Evaluate this sentence for spelling and grammar mistakes</td>\n",
       "      <td>He finnished his meal and left the resturant</td>\n",
       "      <td>He finished his meal and left the restaurant.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How did Julius Caesar die?</td>\n",
       "      <td></td>\n",
       "      <td>Julius Caesar was assassinated on the Ides of March in 44 BC.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   instruction  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Give three tips for staying healthy.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           What are the three primary colors?   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Describe the structure of an atom.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             How can we reduce air pollution?   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Pretend you are a project manager of a construction company. Describe a time when you had to make a difficult decision.   \n",
       "5  Write a concise summary of the following:\\n\"Commodore 64 (commonly known as the C64 or CBM 64) was manufactured by Commodore Business Machine (CBM) in August 1982 with a starting price of $595. It was an 8-bit home computer with remarkable market success. Between 1983-1986, C64 sales amounted to about 17 million units sold, making them the best-selling single personal computer model of all time in 1983-1986. \\n\\nAdditionally, the Commodore 64 dominated the market with between 30% and 40% share and 2 million units sold per year, outselling the IBM PC clones, Apple Computers, and Atari computers. Adding to their success, Sam Tramiel (former Atari president), during an interview in 1989, said they were building 400,000 C64s a month for a couple of years. \"   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Explain why the following fraction is equivalent to 1/4   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Write a short story in third person narration about a protagonist who has to make an important career decision.   \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Evaluate this sentence for spelling and grammar mistakes   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   How did Julius Caesar die?   \n",
       "\n",
       "                                          input  \\\n",
       "0                                                 \n",
       "1                                                 \n",
       "2                                                 \n",
       "3                                                 \n",
       "4                                                 \n",
       "5                                                 \n",
       "6                                          4/16   \n",
       "7                                                 \n",
       "8  He finnished his meal and left the resturant   \n",
       "9                                                 \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         refine_response  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                              1. Eat a balanced diet.\\n                  2. Exercise regularly.\\n                  3. Get enough sleep.  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    The three primary colors are red, blue, and yellow.  \n",
       "2                                                                                                                                                                                                                                                        An atom is composed of a nucleus, which is made up of protons and neutrons, and electrons, which are arranged in shells around the nucleus. The nucleus is made up of positively charged protons and uncharged neutrons. The electrons are negatively charged particles that orbit the nucleus.  \n",
       "3                                                                                                                                                                                                                           We can reduce air pollution by switching to renewable energy sources, such as solar and wind power. We can also reduce our reliance on fossil fuels by using public transportation, carpooling, and biking. We can also reduce air pollution by planting trees and shrubs, which absorb carbon dioxide and other pollutants.  \n",
       "4                                                                           As a project manager of a construction company, I had to make difficult decisions on a regular basis. One example was when I had to decide whether to continue with a project that was running over budget. I had to weigh the pros and cons of continuing with the project, as well as the potential risks of not completing it. In the end, I decided to continue with the project, as I believed that the benefits of completing it outweighed the risks of not doing so.  \n",
       "5                                                                                                                                        The Commodore 64 (commonly known as the C64 or CBM 64) was manufactured by Commodore Business Machine (CBM) in August 1982 with a starting price of $595. It was an 8-bit home computer with remarkable market success. Between 1983-1986, C64 sales amounted to about 17 million units sold, making them the best-selling single personal computer model of all time in 1983-1986. Additionally, the Commodore  \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                            The fraction 4/16 is equivalent to 1/4 because it can be simplified to 1/4 by dividing both numerator and denominator by 4.  \n",
       "7  Once upon a time, there was a young man named John who had to make an important career decision. He had just graduated from college with a degree in computer science, but he wasn't sure if he wanted to pursue a career in that field. \\n\\nJohn had always been interested in business, but he was afraid to take the leap and pursue a career in that field. He was worried that he wouldn't be able to make enough money to support himself and his family. \\n\\nOne day, John decided to take a leap of faith and pursue a career in business. He  \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          He finished his meal and left the restaurant.  \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Julius Caesar was assassinated on the Ides of March in 44 BC.  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 2000\n",
    "refine_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KoAlpaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "675d824874424556970f3df67d3d5e1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "model_ckpt = \"beomi/KoAlpaca\"\n",
    "\n",
    "KoAlpaca_tokenizer = LlamaTokenizer.from_pretrained(model_ckpt)\n",
    "KoAlpaca_model = LlamaForCausalLM.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context.\\n\"\n",
    "        \"아래는 작업을 설명하는 명령어와 추가적 맥락을 제공하는 입력이 짝을 이루는 예제입니다.\\n\\n\"\n",
    "        \"Write a response that appropriately completes the request.\\n요청을 적절히 완료하는 응답을 작성하세요.\\n\\n\"\n",
    "        \"### Instruction(명령어):\\n{instruction}\\n\\n### Input(입력):\\n{input}\\n\\n### Response(응답):\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task.\\n\"\n",
    "        \"아래는 작업을 설명하는 명령어입니다.\\n\\n\"\n",
    "        \"Write a response that appropriately completes the request.\\n명령어에 따른 요청을 적절히 완료하는 응답을 작성하세요.\\n\\n\"\n",
    "        \"### Instruction(명령어):\\n{instruction}\\n\\n### Response(응답):\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def gen(prompt, user_input=None, max_new_tokens=128, temperature=0.5):\n",
    "    if user_input:\n",
    "        x = PROMPT_DICT['prompt_input'].format(instruction=prompt, input=user_input)\n",
    "    else:\n",
    "        x = PROMPT_DICT['prompt_no_input'].format(instruction=prompt)\n",
    "    print(f\"prompt instruction : {x}\")\n",
    "    \n",
    "    input_ids = KoAlpaca_tokenizer.encode(x, return_tensors=\"pt\")\n",
    "    print(input_ids)\n",
    "\n",
    "    gen_tokens = KoAlpaca_model.generate(\n",
    "        input_ids, \n",
    "        max_new_tokens=max_new_tokens, \n",
    "        num_return_sequences=1, \n",
    "        temperature=temperature,\n",
    "        no_repeat_ngram_size=6,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    print(gen_tokens)\n",
    "    \n",
    "    gen_text = KoAlpaca_tokenizer.decode(gen_tokens[0], skip_special_tokens=True)\n",
    "    \n",
    "    return gen_text.replace(x, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt instruction : Below is an instruction that describes a task, paired with an input that provides further context.\n",
      "아래는 작업을 설명하는 명령어와 추가적 맥락을 제공하는 입력이 짝을 이루는 예제입니다.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "요청을 적절히 완료하는 응답을 작성하세요.\n",
      "\n",
      "### Instruction(명령어):\n",
      "이 문장에 철자와 문법 오류가 있는지 평가하세요.\n",
      "\n",
      "### Input(입력):\n",
      "그는 식사를 마치고 식당을 나섰습니다.\n",
      "\n",
      "### Response(응답):\n",
      "tensor([[    2, 13866,   338,   385, 15278,   393, 16612,   263,  3414, 29892,\n",
      "          3300,  2859,   411,   385,  1881,   393,  8128,  4340,  3030, 29889,\n",
      "            13, 30860,   238,   161,   155, 31081, 29871,   239,   161,   148,\n",
      "           239,   154,   136, 31286, 29871,   239,   135,   167, 31976, 30944,\n",
      "         31081, 29871, 31976,   238,   163,   188, 31129,   239,   156,   131,\n",
      "         29871,   239,   185,   151, 30903,   239,   163,   132, 29871,   238,\n",
      "           170,   168,   238,   160,   192, 31286, 29871, 31306, 31334, 30944,\n",
      "         31081, 29871,   239,   161,   136,   238,   163,   168, 30393, 29871,\n",
      "           239,   170,   160, 31286, 29871, 30393,   238,   166,   171, 31081,\n",
      "         29871,   239,   155,   139, 31306,   239,   161,   136, 31063, 30709,\n",
      "         29889,    13,    13,  6113,   263,  2933,   393,  7128,  2486,  1614,\n",
      "          2167,   278,  2009, 29889,    13, 31527,   239,   181,   176, 31286,\n",
      "         29871,   239,   163,   132,   239,   163,   139,   240,   161,   139,\n",
      "         29871,   239,   156,   135,   238,   166,   143, 30944, 31081, 29871,\n",
      "           239,   160,   148,   238,   142,   184, 31286, 29871,   239,   161,\n",
      "           148, 31126, 30944, 31578, 31527, 29889,    13,    13,  2277, 29937,\n",
      "          2799,  4080, 29898, 31976,   238,   163,   188, 31129,  1125,    13,\n",
      "         30393, 29871, 31406, 31299, 31054, 29871,   239,   181,   163, 31013,\n",
      "           239,   156,   131, 29871, 31406,   238,   181,   152, 29871, 31346,\n",
      "           238,   168,   155, 30903, 29871,   239,   161,   139, 31081, 30811,\n",
      "         29871,   240,   146,   140, 30903, 30944, 31578, 31527, 29889,    13,\n",
      "            13,  2277, 29937, 10567, 29898,   239,   161,   136,   238,   163,\n",
      "           168,  1125,    13, 31607, 31081, 29871, 31895, 30791, 31517, 29871,\n",
      "         31417,   239,   188,   155, 31137, 29871, 31895,   238,   142,   188,\n",
      "         31286, 29871, 31207,   239,   135,   179,   239,   141,   184, 31063,\n",
      "         30709, 29889,    13,    13,  2277, 29937, 13291, 29898,   239,   160,\n",
      "           148,   238,   142,   184,  1125]])\n",
      "tensor([[    2, 13866,   338,   385, 15278,   393, 16612,   263,  3414, 29892,\n",
      "          3300,  2859,   411,   385,  1881,   393,  8128,  4340,  3030, 29889,\n",
      "            13, 30860,   238,   161,   155, 31081, 29871,   239,   161,   148,\n",
      "           239,   154,   136, 31286, 29871,   239,   135,   167, 31976, 30944,\n",
      "         31081, 29871, 31976,   238,   163,   188, 31129,   239,   156,   131,\n",
      "         29871,   239,   185,   151, 30903,   239,   163,   132, 29871,   238,\n",
      "           170,   168,   238,   160,   192, 31286, 29871, 31306, 31334, 30944,\n",
      "         31081, 29871,   239,   161,   136,   238,   163,   168, 30393, 29871,\n",
      "           239,   170,   160, 31286, 29871, 30393,   238,   166,   171, 31081,\n",
      "         29871,   239,   155,   139, 31306,   239,   161,   136, 31063, 30709,\n",
      "         29889,    13,    13,  6113,   263,  2933,   393,  7128,  2486,  1614,\n",
      "          2167,   278,  2009, 29889,    13, 31527,   239,   181,   176, 31286,\n",
      "         29871,   239,   163,   132,   239,   163,   139,   240,   161,   139,\n",
      "         29871,   239,   156,   135,   238,   166,   143, 30944, 31081, 29871,\n",
      "           239,   160,   148,   238,   142,   184, 31286, 29871,   239,   161,\n",
      "           148, 31126, 30944, 31578, 31527, 29889,    13,    13,  2277, 29937,\n",
      "          2799,  4080, 29898, 31976,   238,   163,   188, 31129,  1125,    13,\n",
      "         30393, 29871, 31406, 31299, 31054, 29871,   239,   181,   163, 31013,\n",
      "           239,   156,   131, 29871, 31406,   238,   181,   152, 29871, 31346,\n",
      "           238,   168,   155, 30903, 29871,   239,   161,   139, 31081, 30811,\n",
      "         29871,   240,   146,   140, 30903, 30944, 31578, 31527, 29889,    13,\n",
      "            13,  2277, 29937, 10567, 29898,   239,   161,   136,   238,   163,\n",
      "           168,  1125,    13, 31607, 31081, 29871, 31895, 30791, 31517, 29871,\n",
      "         31417,   239,   188,   155, 31137, 29871, 31895,   238,   142,   188,\n",
      "         31286, 29871, 31207,   239,   135,   179,   239,   141,   184, 31063,\n",
      "         30709, 29889,    13,    13,  2277, 29937, 13291, 29898,   239,   160,\n",
      "           148,   238,   142,   184,  1125,   239,   181,   163, 31013,   239,\n",
      "           163,   132,   239,   159,   191, 30906, 31081, 29871, 31406, 31299,\n",
      "         31354, 29871,   239,   156,   135,   238,   181,   192, 31980, 31063,\n",
      "         30709, 29889, 29871, 31406,   238,   181,   152,   239,   163,   132,\n",
      "           239,   159,   192, 31063, 30709, 29889,     2]])\n",
      "철자적으로는 문장은 완벽합니다. 문법적윽니다.\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "prompt = \"이 문장에 철자와 문법 오류가 있는지 평가하세요.\"\n",
    "user_input = \"그는 식사를 마치고 식당을 나섰습니다.\"\n",
    "generated_text = gen(prompt, user_input)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "corn",
   "language": "python",
   "name": "corn"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
